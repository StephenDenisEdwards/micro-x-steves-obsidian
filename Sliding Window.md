# Sliding Window

A **sliding window** is the simplest [[Compaction Strategies|compaction strategy]]: keep only the most recent N messages (or N tokens) and drop everything older. It's fast, predictable, and requires no LLM calls.

## How It Works

```
Window size: 5 messages

Turn 1: [User] "Set up the project"         ← dropped
Turn 2: [Agent] Creates project structure    ← dropped
Turn 3: [User] "Add authentication"          ← dropped
Turn 4: [Agent] Implements auth              ← kept (oldest in window)
Turn 5: [User] "Now add rate limiting"       ← kept
Turn 6: [Agent] Reads existing middleware     ← kept
Turn 7: [Agent] Implements rate limiter      ← kept
Turn 8: [User] "Add tests"                  ← kept (newest)
```

The window slides forward as new messages arrive. Messages that fall off the back are permanently lost.

## Window Sizing

Windows can be defined by:

- **Message count**: Keep last K messages. Simple but ignores that messages vary wildly in size.
- **Token count**: Keep messages that fit within T tokens. More precise, requires [[Token Counting]].
- **Turn count**: Keep last K user-agent turn pairs. Preserves conversational coherence.

Token-based windows are preferred for production use because they give predictable [[Context Window]] consumption.

## Strengths

- **Zero latency overhead**: No LLM calls, no processing — just slice the array
- **Zero cost**: No additional API calls
- **Predictable memory usage**: Context window consumption is bounded
- **Simple to implement**: A few lines of code

```python
def sliding_window(messages, max_tokens):
    total = 0
    window = []
    for msg in reversed(messages):
        msg_tokens = count_tokens(msg)
        if total + msg_tokens > max_tokens:
            break
        window.insert(0, msg)
        total += msg_tokens
    return window
```

## Weaknesses

- **Complete information loss**: Dropped messages are gone. If the agent decided something important in Turn 2, it won't remember after Turn 2 leaves the window.
- **No selectivity**: A critical tool result and a trivial acknowledgment are treated the same. [[Selective Pruning]] solves this.
- **Context discontinuity**: The agent suddenly "forgets" earlier context, which can cause it to repeat work or contradict earlier decisions.

## When to Use

Sliding window works well when:
- Tasks are short-lived and recent context is sufficient
- You're building a chatbot (not a multi-step agent)
- Latency is the top priority
- You're prototyping before implementing [[Summarization-Based Compaction]]

## Hybrid Approaches

The most common improvement is **sliding window + summary**:

```
[System prompt]
[Summary of turns 1-15]     ← generated by summarization
[Turn 16]                    ← start of sliding window
[Turn 17]
[Turn 18]
[Turn 19]
[Turn 20]                    ← current turn
```

This is exactly what [[Hierarchical Memory]] formalizes, and what [[Compaction in LangChain]]'s `ConversationSummaryBufferMemory` implements.

See also: [[Compaction Strategies]], [[Context Window]], [[Compaction Tradeoffs]]
