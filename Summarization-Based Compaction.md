# Summarization-Based Compaction

Summarization-based compaction uses an LLM to compress older conversation turns into a concise summary. It's the most popular [[Compaction Strategies|compaction strategy]] because it preserves semantic content while dramatically reducing token count.

## How It Works

```
Before compaction (12,000 tokens):
  [Turn 1] User asks about auth options
  [Turn 2] Agent researches OAuth vs API keys
  [Turn 3] Agent reads OAuth library docs
  [Turn 4] Agent implements OAuth flow
  [Turn 5] User requests error handling
  [Turn 6] Agent adds try/catch blocks
  [Turn 7] User asks for tests      ← compaction trigger

After compaction (~4,000 tokens):
  [Summary] "Previously: implemented OAuth authentication flow with
   error handling. User requested OAuth over API keys. Agent used
   the oauth-lib library. Key files modified: auth.py, config.py."
  [Turn 7] User asks for tests      ← recent turns kept verbatim
```

The summary is generated by sending the older turns to an LLM with a prompt like: "Summarize this conversation history, preserving key decisions, file paths, and technical details."

## Summary Quality

The quality of the summary determines how well the agent performs after compaction. Key practices:

- **Preserve concrete details**: file paths, variable names, error messages, decisions made
- **Preserve the "why"**: rationale behind choices matters more than the back-and-forth
- **Use structured summaries**: bullet points > prose for preserving distinct facts
- **Include [[Tool Call History]] results**: tool outputs often contain critical data

A poor summary loses facts the agent needs later. A good summary is denser than the original — more information per token.

## Recursive Summarization

For very long tasks, summaries themselves may need compaction. Recursive summarization creates a "summary of summaries":

```
Turns 1-10  → Summary A (500 tokens)
Turns 11-20 → Summary B (500 tokens)
Turns 21-30 → Summary C (500 tokens)

When A + B + C gets too large:
A + B + C → Meta-Summary (300 tokens)
```

This is the foundation of [[Hierarchical Memory]], which formalizes the layering.

## Cost Considerations

Every summarization step requires an LLM call:
- Using the same model (e.g., GPT-4o): expensive but high-quality summaries
- Using a smaller model (e.g., GPT-4o-mini): cheaper, slightly lower quality
- Using a local model: free but may miss nuances

See [[Compaction Tradeoffs]] for detailed cost analysis. [[Compaction in LangChain]] provides `ConversationSummaryMemory` which handles this automatically.

## Comparison with Other Strategies

| | Summarization | [[Sliding Window]] | [[Selective Pruning]] |
|---|---|---|---|
| Information loss | Medium | High | Low-Medium |
| Latency overhead | High (LLM call) | None | Low |
| Cost | Higher | None | None |
| Best for | Multi-phase tasks | Short tasks | Mixed-importance turns |

See also: [[Compaction Strategies]], [[Context Window]], [[Token Counting]]
